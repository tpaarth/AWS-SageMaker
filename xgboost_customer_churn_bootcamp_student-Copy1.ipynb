{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Data Science with Amazon SageMaker\n",
    "_**Using Amazon SageMaker to solve an end-to-end machine learning problem**_\n",
    "\n",
    "---\n",
    "\n",
    "Learn to use Amazon SageMaker for creating, tuning, and deploying a machine learning (ML) model for predicting customer churn of a moblile phone service provider.\n",
    "\n",
    "## Modules\n",
    "\n",
    "1. [Prepare a dataset for training](#Prepare-a-dataset-for-training)\n",
    "2. [Train and evaluate a model](#Train-and-evaluate-a-model)\n",
    "3. [Automatically tune the model](#Automatically-tune-the-model)\n",
    "4. [Make the model production ready](#Make-the-model-production-ready)\n",
    "5. [AWS Auto Scaling](#AWS-Auto-Scaling)\n",
    "6. [Relative cost of errors](#Relative-cost-of-errors)\n",
    "  \n",
    "---\n",
    "\n",
    "## Prepare a dataset for training\n",
    "\n",
    "### Churn prediction problem\n",
    "\n",
    "Losing customers is costly for any business.  Identifying unhappy customers early gives you a chance to offer them incentives to stay.  You will use ML to automate the identification of unhappy customers, also known as *customer churn prediction*. \n",
    "\n",
    "ML models rarely give perfect predictions, so you will learn how to adjust for prediction mistakes in your total ML costs.\n",
    "\n",
    "The example of churn that may be familiar to you; leaving a mobile phone service provider for a competitor.  If a provider is aware that a particular customer is considering leaving, it can offer timely incentives, perhaps in the form of a phone upgrade, to encourage the customer to continue service.  \n",
    "\n",
    "Incentives are often more cost-effective than losing and reacquiring a customer.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Mobile phone service providers keep historical records on customers who churn bit ultimately continue using the service. You can use this data to construct an ML model of one mobile phone provider's churn using a process called *training*. \n",
    "\n",
    "You'll train the model, can pass the profile information of an arbitrary customer to the model, and have the model predict whether this customer is going to churn. The model will make mistakes&mdash;predicting the future is tricky. However, you will learn how to manage prediction errors.\n",
    "\n",
    "The dataset used is publicly available and is mentioned in the book [*Discovering Knowledge in Data*](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. The author attributed the dataset to the University of California Irvine Repository of Machine Learning Datasets.\n",
    "\n",
    "As part of the lab setup, the relevant dataset, churn.txt has been downloaded and made available for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head './churn.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks like a CSV file with a header row. \n",
    "\n",
    "You will use the pandas library for loading and displaying this raw dataset. \n",
    "\n",
    "The resulting `churn` variable is a pandas [DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "churn = pd.read_csv('./churn.txt')\n",
    "churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look. By modern standards, it’s a relatively small dataset with only 3,333 records. \n",
    "\n",
    "Each record uses 21 attributes to describe the profile of a customer of an unknown US mobile phone service provider. \n",
    "\n",
    "The attributes are:\n",
    "\n",
    "- `State`: The US state in which the customer resides indicated by a two-letter abbreviation. For example, OH or NJ\n",
    "- `Account Length`: The number of days that this account has been active\n",
    "- `Area Code`: The three-digit area code of the corresponding customer’s phone number\n",
    "- `Phone`: The seven-digit phone number\n",
    "- `Int’l Plan`: Whether the customer has an international calling plan: yes/no\n",
    "- `VMail Plan`: Whether the customer has a voice mail feature: yes/no\n",
    "- `VMail Message`: The average number of voice mail messages per month\n",
    "- `Day Mins`: The total number of calling minutes used during the day\n",
    "- `Day Calls`: The total number of calls placed during the day\n",
    "- `Day Charge`: The billed cost of daytime calls\n",
    "- `Eve Mins, Eve Calls, Eve Charge`: The billed cost for calls placed during the evening\n",
    "- `Night Mins`, `Night Calls`, `Night Charge`: The billed cost for calls placed during nighttime\n",
    "- `Intl Mins`, `Intl Calls`, `Intl Charge`: The billed cost for international calls\n",
    "- `CustServ Calls`: The number of calls placed to Customer Service\n",
    "- `Churn?`: Whether the customer left the service: true/false\n",
    "\n",
    "The last attribute, `Churn?`, is known as the *target attribute*&mdash;the attribute that we want the ML model to predict.  Because the target attribute is binary, our model will be performing binary prediction, also known as *binary classification*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will explore and visualize the data and the relationships between attributes.\n",
    "\n",
    "We've already used pandas.  We also recommend using the numpy and/or matplotlib libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to use a histogram to see how the values of individual attributes are distributed, as well as compute summary statistics for numeric attributes such as mean, min values, max values, standard deviations, etc. \n",
    "\n",
    "For categorical variables we need to see frequency tables. \n",
    "\n",
    "While there are several ways to do this in Python&mdash;we are going to use the following pandas functions:[`hist()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.hist.html#pandas.DataFrame.hist),[`describe()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html#pandas.DataFrame.describe),[`crosstab()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html), and[`select_dtypes()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.select_dtypes.html). \n",
    "\n",
    "To show the histograms right in the Jupyter notebook, we will also use the[`%matplotlib inline`](https://ipython.readthedocs.io/en/stable/interactive/plotting.html) magic function of IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show frequency tables for each categorical feature and counts of unique values\n",
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=churn[column],\n",
    "                        columns='% observations', \n",
    "                        normalize='columns'))\n",
    "    print(\"# of unique values {}\".format(churn[column].nunique()))\n",
    "\n",
    "# show summary statistics\n",
    "display(churn.describe())\n",
    "\n",
    "# build histograms for each numeric feature\n",
    "%matplotlib inline\n",
    "hist = churn.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see immediately that:\n",
    "* `State` appears to be quite evenly distributed\n",
    "* `Phone` takes on too many unique values to be of any practical use.  It's possible that parsing out the prefix could have some value, but unless you have more context on how these are allocated, avoid using it.\n",
    "* Only 14% of customers churned, so there is some class imabalance, but nothing extreme.\n",
    "* Most of the numeric features are surprisingly well distributed, with many showing bell-like gaussianity.  `VMail Message` being a notable exception (and `Area Code` showing up as a feature we should convert to non-numeric).\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Now that we've shown how to do the first part of the exercise, it's time for you to complete the second part by looking at how each feature relates to our target variable `Churn?`. \n",
    "\n",
    "You can accomplish this using the same pandas functions of `crosstab()`and`hist()`by entering the code in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution goes here... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your analysis, your solution might show different results, but you should see that churners appear:\n",
    "* Fairly evenly distributed geographically\n",
    "* More likely to have an international plan\n",
    "* Less likely to have a voicemail plan\n",
    "* Exhibit some bimodality in daily minutes (either higher or lower than the average for non-churners)\n",
    "* Larger number of customer service calls (which makes sense as we'd expect customers who experience lots of problems may be more likely to churn)\n",
    "\n",
    "In addition, we see that churners take on very similar distributions for features like`Day Mins`and`Day Charge`.  \n",
    "\n",
    "That's not surprising; you would expect minutes spent talking to correlate with charges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Examine the pairwise relationships between attributes to see how they are correlated. Use the pandas functions [corr()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) and [scatter_matrix()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.plotting.scatter_matrix.html) pandas functions to achieve this. \n",
    "\n",
    "Enter your solution in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution goes here... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see several features that essentially have 100 percent correlation with one another.  Including these feature pairs in some machine learning algorithms can create catastrophic problems, while in others it will only introduce minor redundancy and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for model training, first remove the columns that observed as useless for our purposes. \n",
    "\n",
    "Remove the`Phone`and`Area Code`attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop(['Phone', 'Area Code'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, remove one feature from each of the highly correlated pairs: \n",
    "\n",
    "* `Day Charge` from the pair with `Day Mins`\n",
    "* `Night Charge` from the pair with `Night Mins`\n",
    "* `Intl Charge` from the pair with `Intl Mins`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop(['Day Charge', 'Eve Charge',\n",
    "                    'Night Charge', 'Intl Charge'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done all of the changes to the data, let's have a last look into how the data looks like before moving to training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, determine which algorithm to use.  As mentioned above, there appear to be some variables where both high and low (but not intermediate) values are predictive of churn.  In order to accommodate this in an algorithm like linear regression, we'd need to generate polynomial (or *bucketed*) terms.  \n",
    "\n",
    "Instead, let's attempt to model this problem using gradient boosted trees.  \n",
    "\n",
    "Amazon SageMaker provides an XGBoost container that we can use to train in a managed, distributed setting, and then host as a real-time prediction endpoint.  XGBoost uses gradient boosted trees that naturally account for non-linear relationships between features and the target variable, as well as accommodating complex interactions between features.\n",
    "\n",
    "Amazon SageMaker XGBoost can train on data in either a CSV or LibSVM format.  For this example, we'll stick with CSV.  It should:\n",
    "* Have the predictor variable in the first column\n",
    "* Not have a header row\n",
    "\n",
    "First, convert the categorical features into numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.get_dummies(churn)\n",
    "model_data = pd.concat([model_data['Churn?_True.'], model_data.drop(\n",
    "    ['Churn?_False.', 'Churn?_True.'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the data into training, validation, and test sets.  This will help prevent overfitting the model and allow you to test the model's accuracy on data it hasn't already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(\n",
    "    frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload these files to Amazon S3, where SageMaker expects them.\n",
    "\n",
    "boto is the standard Python library used to invoke AWS API. The same S3 bucket and prefix will be used for training and model data.  \n",
    "\n",
    "This should be within the same region as the Notebook Instance, training, and hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'bootcamp-xgboost-churn'\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "    os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(\n",
    "    prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Next, specify the locations of the XGBoost algorithm containers.\n",
    "\n",
    "Amazon SageMaker algorithms are packaged as Docker images. This gives you the flexibility to use almost any algorithm code with Amazon SageMaker, regardless of implementation language, dependent libraries, frameworks, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the IAM Role\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "# Get the XGBoost docker image\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "display(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SageMaker Python SDK](https://sagemaker.readthedocs.io/en/latest/) provides these high-level abstractions for working with Amazon SageMaker:\n",
    "\n",
    "* Estimators: Encapsulates training on SageMaker.\n",
    "* Models: Encapsulates built ML models.\n",
    "* Predictors: Provides real-time inference and transformation using Python data-types against a SageMaker endpoint.\n",
    "* Session: Provides a collection of methods for working with SageMaker resources.\n",
    "\n",
    "Start by creating the [xgboost Estimator](https://sagemaker.readthedocs.io/en/latest/estimators.html). The mandatory paramters are: image_name, role, session, instance_type, and instance_count. \n",
    "\n",
    "For this training job, use:\n",
    "\n",
    "* `image_name = container`\n",
    "* `role=role`\n",
    "* `sagemaker_session = sess`\n",
    "* `train_instance_count = 1`\n",
    "* `train_instance_type = ml.m4.xlarge` \n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Finish the Estimator definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the SageMaker Estimator object\n",
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ML algorithm is configured and tuned based on its hyperparameters, which change the way the algorithem works.\n",
    "\n",
    "The XGBoost hyperparamaters are described in the [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
    "\n",
    "For this example, the required hyperparameters for XGBoost are:\n",
    "\n",
    "* `objective` - Specifies the learning task and the corresponding learning objective. please use **binary:logistic** for binary classification task.  \n",
    "* `num_round` - controls the number of rounds. Each round is trained using the output from the previous one. More rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "\n",
    "A few other key hyperparameters are:\n",
    "\n",
    "* `max_depth` controls how deep each tree within the algorithm can be built. Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting. There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "* `subsample` controls sampling of the training data. This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "* `eta` controls how aggressive each round of boosting is. Larger values lead to more conservative boosting.\n",
    "* `gamma` controls how aggressively trees are grown. Larger values lead to more conservative models.\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Use xgb.set_hyperparameters to set the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "xgb.set_hyperparameters(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because you're training with the CSV file format, create s3_inputs the training function can use as a pointer to the files in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the data inputs\n",
    "s3_input_train = sagemaker.s3_input(\n",
    "    s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(\n",
    "    s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you are ready to train. \n",
    "\n",
    "To train, use the fit() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "\n",
    "Now create a model and deploy it to a hosted endpoint using the deploy API of SageMaker [estimator](https://sagemaker.readthedocs.io/en/latest/estimators.html).  \n",
    "\n",
    "### Exercise 5 \n",
    "Configure `initial_instance_count = 1` and `instance_type = ml.m4.xlarge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "Variant = 'AllTraffic' \n",
    "# Configure the default deployment variant\n",
    "xgb_predictor = xgb.deploy(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Now make real-time predictions from your model by making an http POST request.  \n",
    "\n",
    "First, setup serializers and deserializers for passing the`test_data` arrays to the model that will be behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a simple function to:\n",
    "\n",
    "1. Loop over your test dataset\n",
    "2. Split it into mini-batches of rows \n",
    "3. Convert those mini-batchs to CSV string payloads\n",
    "4. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "5. Collect predictions and convert from the CSV output your model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join(\n",
    "            [predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "\n",
    "predictions = predict(test_data.as_matrix()[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of your ML model by comparing actual values to predicted values.\n",
    "\n",
    "In this case, you will predict whether the customer churned (`1`) or not (`0`), which produces a simple confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(\n",
    "    predictions), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Due to randomized elements of the algorithm, your results may differ slightly._\n",
    "\n",
    "Of the 48 churners, 39 were correcly predicted. (True positives.) Four were incorrectly predited; they ended up not churning. (False postives.) \n",
    "\n",
    "Another nine customers did end up churning when we predicted they would not.  (False negatives.)\n",
    "\n",
    "**Important:** Because of the`np.round()`function above, we are using a simple threshold (or cutoff) of 0.5.  \n",
    "\n",
    "Our predictions from `xgboost` come out as continuous values between 0 and 1 and are forced into the binary classes that we began with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically tune the model\n",
    "\n",
    "### Model hyperparameter tuning for XGBoost\n",
    "\n",
    "Automatic model tuning, or *hyperparameter tuning*, finds the best version of a model by running many jobs that test a range of hyperparameters on your dataset. \n",
    "\n",
    "You choose the tunable hyperparameters, a range of values for each, and an objective metric. Objective metrics are chosen from the metrics that the algorithm computes. \n",
    "\n",
    "Automatic model tuning searches the hyperparameters chosen to find the combination of values that result in the model that optimizes the objective metric.\n",
    "\n",
    "For more information about model tuning, see [Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "The HyperparameterTuner object takes the Estimator to obtain each job configuration information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the Estimator from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each tuning job, you can configure a set of static hyperparameters and range of tuneable hyperparameters.  \n",
    "\n",
    "Use the IntegerParameter, CategoricalParameter, and ContinuousParameter objects to assign values to the hyperparameter_ranges variable.  (This variable is a dictionary data type.)\n",
    "\n",
    "### Exercise 7\n",
    "\n",
    "Set the ranges for the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "\n",
    "xgb.set_hyperparameters(objective='binary:logistic',\n",
    "                        num_round=10)\n",
    "hyperparameter_ranges = { \n",
    "    ## Your solution goes here...  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify the objective metric to tune and its definition.  The definition includes the regular expression (regex) needed to extract that metric from the CloudWatch logs of the training job. \n",
    "\n",
    "The XGBoost algorithm computes nine [metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html) during training that can be used as objective metrics.\n",
    "\n",
    "In this case, you only need to specify the metric name.  You do not need to provide regex. \n",
    "\n",
    "### Exercise 8\n",
    "\n",
    "Choose one of the metrics as an objective. Choose an objective that will fit a binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compete the statement\n",
    "objective_metric_name = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a HyperparameterTuner object, to which we pass:\n",
    "\n",
    "* XGBoost estimator\n",
    "* Hyperparameter ranges \n",
    "* Objective metric name\n",
    "* Tunning job configuration\n",
    "\n",
    "### Exercise 9\n",
    "\n",
    "Create the [HyperparameterTuner](https://sagemaker.readthedocs.io/en/latest/tuner.html) object. \n",
    "\n",
    "The mandatory parameters are: \n",
    "\n",
    "* `estimator`\n",
    "* `objective_metric_name`\n",
    "* `hyperparameter_ranges`\n",
    "* `objective_type`\n",
    "* `max_jobs` \n",
    "* `max_parallel_jobs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tunning job\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "## Your soloution goes here\n",
    "tuner = HyperparameterTuner(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the training model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the model using the Hyperparameter tunning job, deploy the new model to the endpoint created previously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the new trained model\n",
    "\n",
    "Amazon SageMaker includes built-in A/B testing capabilities that help you test your model and experiment with different versions to achieve the best results.  \n",
    "\n",
    "Start by examining the results of the tuning job.\n",
    "\n",
    "To monitor the progress and completion of the hyperparameter tuning job, check the section “Hyperparameter tuning jobs” section of the SageMaker console. \n",
    "\n",
    "This cell checks the status of hyperparameter tuning job, programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "sage_client = sess.sagemaker_client\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "\n",
    "# run this cell to check current status of hyperparameter tuning job\n",
    "tuning_job_result = sage_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name)\n",
    "\n",
    "status = tuning_job_result['HyperParameterTuningJobStatus']\n",
    "if status != 'Completed':\n",
    "    print(\"The tuning job has not been completed.\")\n",
    "\n",
    "job_count = tuning_job_result['TrainingJobStatusCounters']['Completed']\n",
    "print(\"%d training jobs have completed\\n\" % job_count)\n",
    "\n",
    "if tuning_job_result.get('BestTrainingJob', None):\n",
    "    print(\"Best model found so far:\")\n",
    "    pprint(tuning_job_result['BestTrainingJob'])\n",
    "else:\n",
    "    print(\"No training jobs have reported results yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training jobs have completed, deploy the best model to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker model from the best model\n",
    "model_name = sess.create_model_from_job(training_job_name=tuner.best_training_job(),\n",
    "                                        role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model, create a new SageMaker endpoint configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current endpoint configuration\n",
    "endpoint = sage_client.describe_endpoint(EndpointName=xgb_predictor.endpoint)\n",
    "endpoint_config = sage_client.describe_endpoint_config(\n",
    "    EndpointConfigName=endpoint['EndpointConfigName'])\n",
    "\n",
    "# Change the current deployment weight to 0.5 (we'll move 50% of the traffic to the new model)\n",
    "current_model_config = endpoint_config['ProductionVariants'][0]\n",
    "current_model_config['InitialVariantWeight'] = 0.5\n",
    "current_model_config['VariantName'] = 'XGBoost'\n",
    "Variant = 'TunedXGBoost'\n",
    "\n",
    "tuned_model_config = {'ModelName': model_name,\n",
    "                      'InitialInstanceCount': 1,\n",
    "                      'InstanceType': 'ml.m4.xlarge',\n",
    "                      'VariantName': Variant,\n",
    "                      'InitialVariantWeight': 0.5}\n",
    "\n",
    "# Create the new endpoint configuration\n",
    "sage_client.create_endpoint_config(\n",
    "    EndpointConfigName='AB-Config',\n",
    "    ProductionVariants=[current_model_config,\n",
    "                        tuned_model_config])\n",
    "\n",
    "# Update the endpoint\n",
    "sage_client.update_endpoint(\n",
    "    EndpointName=endpoint['EndpointConfigName'],\n",
    "    EndpointConfigName='AB-Config'\n",
    ")\n",
    "result = sess.wait_for_endpoint(endpoint['EndpointConfigName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the SageMaker console, you'll be able to see that the endpoint now sends 50 percent of the traffic to the old model and 50 percent of it to the new one.  \n",
    "\n",
    "Create a new crosstab and make sure that everything works as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join(\n",
    "            [predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.as_matrix()[:, 1:])\n",
    "\n",
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(\n",
    "    predictions), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it worked as expected (no errors), you can send all traffic to the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_client.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpoint['EndpointConfigName'],\n",
    "    DesiredWeightsAndCapacities=[\n",
    "        {\n",
    "            'VariantName': Variant,\n",
    "            'DesiredWeight': 1\n",
    "        },\n",
    "        {\n",
    "            'VariantName': 'XGBoost',\n",
    "            'DesiredWeight': 0\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "response = sess.wait_for_endpoint(endpoint['EndpointConfigName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a new crosstab for the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join(\n",
    "            [predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.as_matrix()[:, 1:])\n",
    "\n",
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(\n",
    "    predictions), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the model production-ready\n",
    "\n",
    "###  AWS Auto Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your endpoint goes into production, depending on your needs and expected traffic, throughput from a single endpoint might result in a bad experience under high load for users. \n",
    "\n",
    "Under heavy load, an endpoint behaves like a web server. When requests take more time to fulfill (the throughput of the endpoint is decreasing), the endpoint could start issuing errors due to timeouts and regular instance metrics (such as CPU utilization) will reach maximum utilization. \n",
    "\n",
    "In order to mitigate such a situation, you could provision more instances to back the endpoint serving your model, but this will not dynamically adapt to the traffic and load you are receiving. \n",
    "\n",
    "With AWS Auto Scaling for Amazon SageMaker, you don't have to closely monitor inference volume and manually change the endpoint configuration. Instead, configure a scaling policy to be used by AWS Auto Scaling. \n",
    "\n",
    "Auto Scaling adjusts the number of instances up or down in response to actual workloads, determined by using Amazon CloudWatch metrics and target values defined in the policy. \n",
    "\n",
    "Before putting AWS Auto Scaling in place for your last endpoint, first monitor how the endpoint behaves under load to understand which metrics to track. \n",
    "\n",
    "Create two helper functions that will be in charge of load testing the endpoint by sending requests with some sample data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import time\n",
    "\n",
    "NB_CONNECTIONS = 200\n",
    "\n",
    "# Update number of connections in the pool accepted by the sagemaker runtime client\n",
    "client = boto3.client('sagemaker-runtime',\n",
    "                      config=botocore.client.Config(max_pool_connections=NB_CONNECTIONS))\n",
    "sess.sagemaker_runtime_client = client\n",
    "\n",
    "# Send data for prediction to endpoint\n",
    "def send_data():\n",
    "    return xgb_predictor.predict([i for i in range(66)])\n",
    "\n",
    "# Send nb_requests to endpoint nb_repetitions\n",
    "def load_test_endpoint(nb_requests, nb_repetitions):\n",
    "    for _ in range(nb_repetitions):\n",
    "        out = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=NB_CONNECTIONS) as executor:\n",
    "            future_submit = (executor.submit(send_data)\n",
    "                             for _ in range(nb_requests))\n",
    "            time1 = time.time()\n",
    "            for future in concurrent.futures.as_completed(future_submit):\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                except Exception as exc:\n",
    "                    data = str(type(exc))\n",
    "                finally:\n",
    "                    out.append(data)\n",
    "\n",
    "                    print(str(len(out)), end=\"\\r\")\n",
    "\n",
    "            time2 = time.time()\n",
    "\n",
    "        # Print time taken for nb_requests\n",
    "        print(f'Took {time2-time1:.2f} s for {nb_requests} requests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the helper function and send a batch of 200 requests, with 180 repetitions of this batch.  This should take, on average, less than a second.  The total run time will be just under 3 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_test_endpoint(nb_requests=200, nb_repetitions=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your endpoint under load by testing it with 50k requests. \n",
    "\n",
    "Keep it short, with only 1 repetition of the batch of 50k requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test_endpoint(nb_requests=50000, nb_repetitions=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have enough data points for measuring performance of the endpoint and you can display them using CloudWatch Metrics. \n",
    "\n",
    "Explore the different metrics available for your SageMaker endpoint, and make sure you can see metrics generated by the first load test you performed. \n",
    "\n",
    "On the SageMaker console, navigate to your endpoint and check out the links in the monitor section. Alternatively, run the following cell and open the link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "link = 'https://' + boto3.Session().region_name + '.console.aws.amazon.com/cloudwatch/home?region=' + boto3.Session().region_name + \\\n",
    "    '#metricsV2:namespace=AWS/SageMaker;dimensions=EndpointName,VariantName;search=' + \\\n",
    "    xgb_predictor.endpoint\n",
    "\n",
    "md(\"**Caution**: *Ctrl + click* the link to not loose the current tab with the notebook: [%s](%s)\" % (link, link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that you can closely monitor the performance your endpoint, create a dashboard in Amazon CloudWatch with some of the metrics that you found above, set a scale of one hour's worth of data, and, if you prefer, set auto-refresh to avoid manually refreshing the dashboard.\n",
    "\n",
    "Alternatively, we have provided a template Amazon CloudWatch dashboard you can automatically deploy using AWS CloudFormation. \n",
    "\n",
    "Download the template by opening `dashboard/template-cloudwatch.yaml` from the current folder. Then head over to AWS CloudFormation, and select 'Create Stack', and then 'Upload a template to Amazon S3' by selecting the downloaded template.\n",
    "\n",
    "You will need the endpoint and variant name when you deploy it using AWS CloudFormation. Once created, you can look into the result of the deployment for a direct URL to the dashboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_name = Variant\n",
    "\n",
    "print(\"Endpoint name: \" + xgb_predictor.endpoint)\n",
    "print(\"Variant name: \" + variant_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that most metrics went up with the final bigger batch of requests. Let's say that this endpoint is serving a large number of requests on a regular basis to determine whether customers are likely to churn, and we want to define a reasonable value for the desired throughput to sustain. We could measure this in Requests Per Minute (or RPS), and define that the endpoint needs to sustain up to 10,000 requests to provide answers to all customers before they experience delays or even errors.\n",
    "\n",
    "Below we have defined a simple formula to compute the throughput value used to auto scale following the recommendations in the [load testing guide](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RPS = 10000 \n",
    "SAFETY_FACTOR = .5\n",
    "\n",
    "INVOCATIONS_PER_INSTANCE_THRESHOLD = (MAX_RPS * SAFETY_FACTOR)\n",
    "print(INVOCATIONS_PER_INSTANCE_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the safety factor, which is important as you need to account for situations where the endpoint might behave differently than when the load test was performed, and external factors out of your own reach. \n",
    "\n",
    "The result of this formula will be used as the threshold for the number of invocations per instance that when reached, will trigger the endpoint to auto scale and add an additional instance to back the endpoint. \n",
    "\n",
    "To achieve this objective, register a scalable target with AWS Auto Scaling to define what to auto-scale (e.g. the number of instances), as well as the minimum and maximum capacity.  \n",
    "\n",
    "We recommend setting the minimum and maximum capacity to 1 and 3 respectively.\n",
    "\n",
    "### Exercise 10\n",
    "\n",
    "Fill out the details of this first step to set up AWS Auto Scaling. You can use the `register_scalable_target` [API method](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/application-autoscaling.html#ApplicationAutoScaling.Client.register_scalable_target) of the Auto Scaling API, and the pre-computed `resource_name` for the `ResourceId`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoscaling_client = boto3.client('application-autoscaling')\n",
    "resource_name = 'endpoint/' + xgb_predictor.endpoint + '/variant/' + variant_name\n",
    "\n",
    "response = autoscaling_client.register_scalable_target(\n",
    "    #######\n",
    "    ## Your solution goes here...\n",
    "    #######\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the endpoint's number of instances is registered as a scalable target, it's available to automatically scale up and down. \n",
    "\n",
    "To enable this behavior, create a scaling policy that defines what metrics AWS Auto Scaling needs to track and against what value. \n",
    "\n",
    "In this case, the endpoint should scale when the threshold we defined has been reached. \n",
    "\n",
    "### Exercise 11\n",
    "\n",
    "Fill out the following code with the the metric and value you want to track. \n",
    "\n",
    "You can read more about the `put_scaling_policy` [API method](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/application-autoscaling.html#ApplicationAutoScaling.Client.put_scaling_policy) to understand how to complete the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the endpoint under heavy load again and watch AWS Auto Scaling activate.  \n",
    "\n",
    "Send it another batch of 50k requests and 10 repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test_endpoint(nb_requests=50000, nb_repetitions=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the dashboard created earlier, can you see when auto scaling kicked in? \n",
    "\n",
    "The number of invocations per instance should have dropped below the target, and the invocations graph will now display 2 separate lines for the Invocations and InvocationsPerInstance. \n",
    "\n",
    "This clearly indicates that the model is being served from multiple instances and decreases the load.\n",
    "\n",
    "If you let the above cell continue to run for longer, you might witness the endpoint scaling in. It will go back to a single instance. \n",
    "\n",
    "With multiple instances behind an endpoint, you are maintaining a throughput lower on average than the target we set. AWS Auto Scaling is set up to track 5000 requests per minute. An alarm will trigger an auto-scaling policy if too many data points are under 5000, which can be the case here. \n",
    "\n",
    "In a production setting, you would use a more robust load testing tool to perform a true load test and determine the exact throughput (or other metric) that your endpoint can sustain under heavy load until it starts responding with errors. \n",
    "\n",
    "For more information on the topic, a [blog post](https://aws.amazon.com/blogs/machine-learning/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling/) is available.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative cost of errors\n",
    "\n",
    "#### Binary classification cutoff\n",
    "\n",
    "Let us return now to model evaluation and chosing a cutoff value to turn the model prediction into a binary decision. \n",
    "\n",
    "Dealing with a churning customer is more expensive than working to retain a customer who *might* churn&mdash;so consider adjusting this cutoff to minimize costly false negatives.  This may increase the number of false positives, but it should also increase the number of true positives while reducing the number of false negatives.\n",
    "\n",
    "To get a rough intuition here, look at the continuous values of your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous valued predictions from the model tend to skew toward 0 or 1, but there is sufficient mass between 0.1 and 0.9. Adjusting the cutoff should shift a number of customer predictions.  \n",
    "\n",
    "Compare the original confusion matrix with a cutoff of 0.5 to one with a cutoff of 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(\n",
    "    predictions), rownames=['actual'], colnames=['predictions']))\n",
    "display(pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(\n",
    "    predictions > 0.3, 1, 0), rownames=['actual'], colnames=['predictions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that changing the cutoff from 0.5 to 0.3 results in two fewer true negatives, seven more true positives, two additional false positives, and seven fewer false negatives.  \n",
    "\n",
    "The numbers may look small, but that's 5 percent of customers, overall, who are shifting because of a change to the cutoff.  \n",
    "\n",
    "Was this the right decision?  \n",
    "\n",
    "You may end up retaining seven more customers, but you also unnecessarily incentivized two more customers who would have stayed without those incentives.  \n",
    "\n",
    "Determining optimal cutoffs is a key step in properly applying machine learning in a real-world setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing the output\n",
    "\n",
    "Now, let's define a helper function that, in addition to computing the above confusion matrix, also \"normalizes\" it.  \n",
    "\n",
    "Instead of showing the absolute counts, you can now see percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(test_data, predictions, cutoff):\n",
    "    conf_matrix = pd.crosstab(index=np.append(test_data.iloc[:, 0], [0,1]), columns=np.append(np.where(\n",
    "        predictions > cutoff, 1, 0), [0,1]), rownames=['actual'], colnames=['predictions'])\n",
    "    conf_matrix.iat[0,0]-=1\n",
    "    conf_matrix.iat[1,1]-=1\n",
    "    conf_matrix_normalized = conf_matrix/conf_matrix.values.sum()\n",
    "    return conf_matrix_normalized\n",
    "\n",
    "confusion_matrix(test_data, predictions, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking relative cost of errors into account\n",
    "\n",
    "Any practical binary classification problem is likely to produce a similarly sensitive cutoff. That by itself isn’t a problem. After all, if the scores for two classes are really easy to separate, the problem probably isn’t very hard to begin with. You might even be able to solve it with simple rules instead of ML.\n",
    "\n",
    "More important, if you put an ML model into production, *you will have to pay for false postiives and false negatives.*\n",
    "\n",
    "Consider similar costs associated with correct predictions of true positives and true negatives.  The choice of the cutoff affects all four of these statistics. Be mindful of the relative costs to the business for each of these four outcomes per prediction.\n",
    "\n",
    "#### Assigning costs\n",
    "\n",
    "The costs for a mobile phone service provider's churn depend on the specific action that the business takes.\n",
    "\n",
    "Let's make some assumptions here.\n",
    "\n",
    "First, assign the true negatives the cost of \\$0. Your model essentially correctly identified a happy customer in this case, and you don’t need to do anything.\n",
    "\n",
    "False negatives are the most problematic, because they incorrectly predict that a churning customer will stay. You lose the customer and will have to pay all the costs of acquiring a replacement customer. \n",
    "\n",
    "These costs include:\n",
    "\n",
    "* Foregone revenue\n",
    "* Advertising costs\n",
    "* Administrative costs\n",
    "* Point of sale costs\n",
    "* Phone hardware subsidies\n",
    "\n",
    "A quick search on the Internet reveals that such costs typically run in the hundreds of dollars.  So, for the purposes of this example, let's assume \\$500. This is the cost of false negatives.\n",
    "\n",
    "Finally, for customers that your model identifies as churning, let's assume a retention incentive in the amount of \n",
    "\\\\$100. If my provider offered me such a concession, I’d certainly think twice before leaving. This is the cost of both true positive and false positive outcomes. \n",
    "\n",
    "In the case of false positives (the customer is happy, but the model mistakenly predicted churn), we will “waste” the \\\\$100 concession. We probably could have spent that \\\\$100 more effectively, but it's possible that we increased the loyalty of an already loyal customer, so that’s not so bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal cutoff\n",
    "\n",
    "It’s clear that false negatives are substantially more costly than false positives. Instead of optimizing for error based on the number of customers, we should be minimizing a cost function that looks like this:\n",
    "\n",
    "```txt\n",
    "$500 * FN(C) + $0 * TN(C) + $100 * FP(C) + $100 * TP(C)\n",
    "```\n",
    "\n",
    "FN(C) means that the false negative percentage is a function of the cutoff, C, and similar for TN, FP, and TP.  You need to find the cutoff, C, where the result of the expression is smallest.\n",
    "\n",
    "A straightforward way to do this, is to simply run a simulation over a large number of possible cutoffs.  \n",
    "\n",
    "Below, we test 100 possible values in the for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COST_OF_ERRORS = np.array([[0, 100], [500, 100]])\n",
    "\n",
    "cutoffs = np.arange(0.01, 1, 0.01)\n",
    "costs = []\n",
    "for c in cutoffs:\n",
    "    cost = (COST_OF_ERRORS * confusion_matrix(test_data,\n",
    "                                              predictions, c)).values.sum()\n",
    "    costs.append(cost)\n",
    "\n",
    "costs = np.array(costs)\n",
    "plt.plot(cutoffs, costs)\n",
    "plt.show()\n",
    "print('Cost is minimized near a cutoff of:',\n",
    "      cutoffs[np.argmin(costs)], 'for a cost of:', np.min(costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart shows that when you set a threshold that's too low, your costs skyrocket because all customers are given a retention incentive.\n",
    "\n",
    "Meanwhile, setting the threshold too high results in too many lost customers, which is, ultimately, nearly as costly.\n",
    "\n",
    "The overall cost can be minimized to \\$24.55 by setting the cutoff to 0.29.\n",
    "\n",
    "How does this compare not using an ML model at all?  That is, not giving any incentives and losing all customers that would churn.\n",
    "\n",
    "Assume that the corresponding confusion matrix looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ml_confusion_matrix = np.array([[0.855086, 0], [1-0.855086, 0]])\n",
    "\n",
    "display((COST_OF_ERRORS * no_ml_confusion_matrix).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost per customer of not using the ML model and while maintaining the same pool of customers is substatially higher: $\\$$72.46 versus $\\$$24.55\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "This notebook showed you how to build a model that predicts whether a customer is likely to churn, and then how to optimally set a threshold that accounts for the cost of true positives, false positives, and false negatives.  \n",
    "\n",
    "There are several ways to extend it including:\n",
    "\n",
    "* Some customers who receive retention incentives will still churn.  Including a probability of churning despite receiving an incentive in your cost function would provide a better ROI on our retention programs.\n",
    "* Customers who switch to a lower-priced plan or who deactivate a paid feature represent different kinds of churn that could be modeled separately.\n",
    "* Modeling the evolution of customer behavior. If usage is dropping and the number of calls placed to Customer Service is increasing, you are more likely to experience churn then if the trend is the opposite. A customer profile should incorporate behavior trends.\n",
    "* Actual training data and monetary cost assignments could be more complex.\n",
    "* Multiple models for each type of churn could be needed.\n",
    "\n",
    "Regardless of additional complexity, similar principles described in this notebook likely apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up\n",
    "\n",
    "If you're done with this notebook, please run the cell below to remove the hosted endpoint and avoid any charges from any stray instances being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
